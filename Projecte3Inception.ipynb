{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load files and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visdom:  False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from skimage import io,transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from visdom import Visdom\n",
    "viz = Visdom()\n",
    "print(\"visdom: \",viz.check_connection())\n",
    "\n",
    "ROOT = \"Datasets/corel_5k/images/\"\n",
    "dirs = [ROOT+i+\"/\" for i in next(os.walk(ROOT))[1]]\n",
    "files = []\n",
    "[files.extend([i+j for j in next(os.walk(i))[2] if \"jpeg\" in j]) for i in dirs]\n",
    "\n",
    "with open(\"Datasets/corel_5k/labels/training_label\") as f:\n",
    "    train_labels = f.readlines()\n",
    "train_labels = [i.split(\" \")[:] for i in train_labels]\n",
    "train_labels = [[int(j) for j in i if j != '' and j != '\\n']for i in train_labels]\n",
    "random.shuffle(train_labels)\n",
    "train_label = train_labels[:4000]\n",
    "val_label = train_labels[4000:]\n",
    "\n",
    "train_label_dict = {}\n",
    "for i in train_label:\n",
    "    train_label_dict[str(i[0])+\".jpeg\"] = i[1:]\n",
    "    \n",
    "val_label_dict = {}\n",
    "for i in val_label:\n",
    "    val_label_dict[str(i[0])+\".jpeg\"] = i[1:]\n",
    "    \n",
    "with open(\"Datasets/corel_5k/labels/test_label\") as f:\n",
    "    test_labels = f.readlines()\n",
    "test_labels = [i.split(\" \")[:] for i in test_labels]\n",
    "test_labels = [[int(j) for j in i if j != '' and j != '\\n']for i in test_labels]\n",
    "test_label_dict = {}\n",
    "for i in test_labels:\n",
    "    test_label_dict[str(i[0])+\".jpeg\"] = i[1:]\n",
    "    \n",
    "train_pairs = []\n",
    "val_pairs = []\n",
    "test_pairs = []\n",
    "for i in files:\n",
    "    img_name = i.split(\"/\")[-1]\n",
    "    if img_name in val_label_dict.keys():\n",
    "        val_pairs.append((i, val_label_dict[img_name]))\n",
    "    elif img_name in test_label_dict.keys():\n",
    "        test_pairs.append((i, test_label_dict[img_name]))\n",
    "    elif img_name in train_label_dict.keys():\n",
    "        train_pairs.append((i, train_label_dict[img_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COREL_5K(Dataset):\n",
    "    def __init__(self, data, num, trans=None):\n",
    "        super(COREL_5K, self).__init__()\n",
    "        self.data = data\n",
    "        self.num = num\n",
    "        self.trans = trans\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_path, label = self.data[index]\n",
    "        label = np.array(label) - 1 # 减一以匹配矩阵下标\n",
    "        img = io.imread(data_path)\n",
    "        if self.trans: # 保证图像都是相同大小的矩阵\n",
    "            img = self.trans(img)\n",
    "        label = np.sum(np.eye(374)[label], axis=0) # 标签向量\n",
    "        return img, label.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num\n",
    "    \n",
    "    def _gen_noise_image(self, image, noise_rate):\n",
    "        noise_image = np.random.uniform(-0.001, 0.001,(image.shape)).astype('float32')\n",
    "        return noise_rate * noise_image + (1-noise_rate) * image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    def __init__(self, in_channels, out1x1, n1x1To3x3, out1x1To3x3, n1x1To3x3To3x3, n3x3To3x3, out3x3, poolTo1x1out):\n",
    "        super(Inception, self).__init__()\n",
    "        # 1x1 conv branch\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out1x1, kernel_size=1),\n",
    "            nn.BatchNorm2d(out1x1),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        # Pool -> 1x1 conv branch\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, poolTo1x1out, kernel_size=1),\n",
    "            nn.BatchNorm2d(poolTo1x1out),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        # 1x1 -> 3x3 conv branch\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, n1x1To3x3, kernel_size=1),\n",
    "            nn.BatchNorm2d(n1x1To3x3),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n1x1To3x3, out1x1To3x3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out1x1To3x3),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        # 1x1 -> 3x3 -> 3x3 conv branch\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, n1x1To3x3To3x3, kernel_size=1),\n",
    "            nn.BatchNorm2d(n1x1To3x3To3x3),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n1x1To3x3To3x3, n3x3To3x3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(n3x3To3x3),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n3x3To3x3, out3x3, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out3x3),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y1 = self.branch1(x)\n",
    "        y2 = self.branch2(x)\n",
    "        y3 = self.branch3(x)\n",
    "        y4 = self.branch4(x)\n",
    "        return torch.cat([y1,y2,y3,y4], 1)\n",
    "    \n",
    "    \n",
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GoogLeNet, self).__init__()\n",
    "        \n",
    "        self.convert_layer = nn.Sequential(\n",
    "            nn.Conv2d(3,3,kernel_size=1),\n",
    "            nn.MaxPool2d(3, stride=3),\n",
    "            nn.Conv2d(3,3,kernel_size=1),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "        ) \n",
    "        self.pre_layers    = nn.Sequential(\n",
    "            nn.Conv2d(3, 192, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        self.a3 = Inception(192,  64,  96, 128, 16, 16, 32, 32)\n",
    "        self.b3 = Inception(256, 128, 128, 192, 32, 32, 96, 64)\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n",
    "\n",
    "        self.a4 = Inception(480, 192,  96, 208, 16, 16,  48,  64)\n",
    "        self.b4 = Inception(512, 160, 112, 224, 24, 24,  64,  64)\n",
    "        self.c4 = Inception(512, 128, 128, 256, 24, 24,  64,  64)\n",
    "        self.d4 = Inception(512, 112, 144, 288, 32, 32,  64,  64)\n",
    "        self.e4 = Inception(528, 256, 160, 320, 32, 32, 128, 128)\n",
    "\n",
    "        self.a5 = Inception(832, 256, 160, 320, 32, 32, 128, 128)\n",
    "        self.b5 = Inception(832, 384, 192, 384, 48, 48, 128, 128)\n",
    "\n",
    "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
    "        self.linear = nn.Linear(1024, 374)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.convert_layer(x)\n",
    "        out = self.pre_layers(out)\n",
    "        out = self.a3(out)\n",
    "        out = self.b3(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.a4(out)\n",
    "        out = self.b4(out)\n",
    "        out = self.c4(out)\n",
    "        out = self.d4(out)\n",
    "        out = self.e4(out)\n",
    "        out = self.maxpool(out)\n",
    "        out = self.a5(out)\n",
    "        out = self.b5(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "NUM_TRAIN = len(train_pairs)\n",
    "NUM_TEST = len(test_pairs)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((192, 192)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=90),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.3853909028535724, 0.4004333749569167, 0.34717936323577203], [1,1,1]),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((192, 192)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.3853909028535724, 0.4004333749569167, 0.34717936323577203], [1,1,1]),\n",
    "])\n",
    "\n",
    "trainDataset = COREL_5K(train_pairs, NUM_TRAIN, train_transform)\n",
    "train_loader = DataLoader(dataset=trainDataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=20, drop_last=True)\n",
    "\n",
    "valDataset = COREL_5K(val_pairs, NUM_TEST, test_transform)\n",
    "val_loader = DataLoader(dataset=valDataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=20, drop_last=False)\n",
    "\n",
    "testDataset = COREL_5K(test_pairs, NUM_TEST, test_transform)\n",
    "test_loader = DataLoader(dataset=testDataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=20, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {}\n",
    "labels = []\n",
    "[labels.extend(i[1]) for i in train_pairs]\n",
    "[labels.extend(i[1]) for i in test_pairs]\n",
    "[labels.extend(i[1]) for i in val_pairs]\n",
    "for i in labels:\n",
    "    if i in a.keys():\n",
    "        a[i] += 1\n",
    "    else:\n",
    "        a[i] = 1\n",
    "for i in a.keys():\n",
    "    a[i] = 1/a[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "model = GoogLeNet()\n",
    "# model.cuda()\n",
    "critrien = nn.BCEWithLogitsLoss(size_average=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏           | 8/500 [02:57<3:01:37, 22.15s/b]"
     ]
    }
   ],
   "source": [
    "# train\n",
    "NUM_EPOCHS = 10\n",
    "best_acc = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    train_acc = 0\n",
    "    test_acc = 0\n",
    "    model.train()\n",
    "    for i, (data, label) in tqdm(enumerate(val_loader), total=NUM_TRAIN // BATCH_SIZE, ncols=50, leave=False, unit='b'):\n",
    "        data = Variable(data)# .cuda()\n",
    "        label = Variable(label)# .cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = critrien(output, label)\n",
    "        train_loss += loss.data[0]\n",
    "        _, predict = torch.max(output, 1)\n",
    "        label = label.cpu().data.numpy()\n",
    "        pred = predict.data\n",
    "        for i in range(len(pred)):\n",
    "            if pred[i] in list(np.where(label[i]==1)[0]):\n",
    "                train_acc += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    for i, (data, label) in enumerate(val_loader):\n",
    "        data = Variable(data)# .cuda()\n",
    "        label = Variable(label)# .cuda()\n",
    "        output = model(data)\n",
    "        loss = critrien(output, label)\n",
    "        test_loss += loss.data[0]\n",
    "        _, predict = torch.max(output, 1)\n",
    "        label = label.cpu().data.numpy()\n",
    "        pred = predict.data\n",
    "        for i in range(len(pred)):\n",
    "            if pred[i] in list(np.where(label[i]==1)[0]):\n",
    "                test_acc += 1\n",
    "    \n",
    "    print('Epoch [%d/%d], Train Loss: %.4f, Train Acc: %.4f, Test Loss: %.4f, Test Acc: %.4f'\n",
    "            %(epoch+1, NUM_EPOCHS, \n",
    "              train_loss / NUM_TRAIN, train_acc / NUM_TRAIN, \n",
    "              test_loss / NUM_TEST, test_acc / NUM_TEST))\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model.state_dict(), \"models/GooLeNet.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "def predict():\n",
    "    with open(\"Datasets/corel_5k/labels/words\") as f:\n",
    "        words = [i[:-1] for i in f.readlines()]\n",
    "\n",
    "    def img_back(img):\n",
    "        mean = [0.3853909028535724, 0.4004333749569167, 0.34717936323577203]\n",
    "        img[:, :, 0] = img[:, :, 0] + mean[0]\n",
    "        img[:, :, 1] = img[:, :, 1] + mean[1]\n",
    "        img[:, :, 2] = img[:, :, 2] + mean[2]\n",
    "        return img\n",
    "\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_TEST = len(test_pairs)\n",
    "\n",
    "    testDataset = COREL_5K(test_pairs, NUM_TEST)\n",
    "    test_loader = DataLoader(dataset=testDataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n",
    "    model = SEResNeXt(BottleneckX, [3, 4, 6, 3], num_classes=374)\n",
    "    model.load_state_dict(torch.load(\"models/SEResNext1.pkl\"))\n",
    "    model.cuda()\n",
    "\n",
    "    test_acc = 0\n",
    "    model.eval()\n",
    "    for i, (data, label) in enumerate(test_loader):\n",
    "        data = Variable(data).cuda()\n",
    "        label = Variable(label).cuda()\n",
    "        output = model(data)\n",
    "    #     _, predict = torch.max(output, 1)\n",
    "        _, predict = torch.sort(output)\n",
    "        label = label.cpu().data.numpy()\n",
    "        pred = (predict.data)[:, -4:]\n",
    "        for i in range(len(pred)):\n",
    "            if len(set(pred[i]) & set(list(np.where(label[i]==1)[0]))):\n",
    "                test_acc += 1\n",
    "            else:\n",
    "                img = data.cpu().data.numpy()[i]\n",
    "                gt = [words[i] for i in list(np.where(label[i]==1)[0])]\n",
    "                predic = [words[i] for i in list(pred[i].cpu().numpy())]\n",
    "                viz.image(np.transpose(img_back(img), (2, 0, 1)),\n",
    "                         opts=dict(title=\" \".join(gt), caption=\" \".join(predic)))\n",
    "    print(test_acc / NUM_TEST)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
