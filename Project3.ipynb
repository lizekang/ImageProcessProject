{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visdom:  False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from skimage import io,transform\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import autograd\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from visdom import Visdom\n",
    "viz = Visdom()\n",
    "print(\"visdom: \",viz.check_connection())\n",
    "\n",
    "ROOT = \"Datasets/corel_5k/images/\"\n",
    "dirs = [ROOT+i+\"/\" for i in next(os.walk(ROOT))[1]]\n",
    "files = []\n",
    "[files.extend([i+j for j in next(os.walk(i))[2] if \"jpeg\" in j]) for i in dirs]\n",
    "\n",
    "with open(\"Datasets/corel_5k/labels/training_label\") as f:\n",
    "    train_labels = f.readlines()\n",
    "train_labels = [i.split(\" \")[:] for i in train_labels]\n",
    "train_labels = [[int(j) for j in i if j != '' and j != '\\n']for i in train_labels]\n",
    "random.shuffle(train_labels)\n",
    "train_label = train_labels[:4000]\n",
    "val_label = train_labels[4000:]\n",
    "\n",
    "train_label_dict = {}\n",
    "for i in train_label:\n",
    "    train_label_dict[str(i[0])+\".jpeg\"] = i[1:]\n",
    "    \n",
    "val_label_dict = {}\n",
    "for i in val_label:\n",
    "    val_label_dict[str(i[0])+\".jpeg\"] = i[1:]\n",
    "    \n",
    "with open(\"Datasets/corel_5k/labels/test_label\") as f:\n",
    "    test_labels = f.readlines()\n",
    "test_labels = [i.split(\" \")[:] for i in test_labels]\n",
    "test_labels = [[int(j) for j in i if j != '' and j != '\\n']for i in test_labels]\n",
    "test_label_dict = {}\n",
    "for i in test_labels:\n",
    "    test_label_dict[str(i[0])+\".jpeg\"] = i[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs = []\n",
    "val_pairs = []\n",
    "test_pairs = []\n",
    "for i in files:\n",
    "    img_name = i.split(\"/\")[-1]\n",
    "    if img_name in val_label_dict.keys():\n",
    "        val_pairs.append((i, val_label_dict[img_name]))\n",
    "    elif img_name in test_label_dict.keys():\n",
    "        test_pairs.append((i, test_label_dict[img_name]))\n",
    "    elif img_name in train_label_dict.keys():\n",
    "        train_pairs.append((i, train_label_dict[img_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COREL_5K(Dataset):\n",
    "    def __init__(self, data, num, trans=None):\n",
    "        super(COREL_5K, self).__init__()\n",
    "        self.data = data\n",
    "        self.num = num\n",
    "        self.trans = trans\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data_path, label = self.data[index]\n",
    "        label = np.array(label) - 1\n",
    "        img = io.imread(data_path)\n",
    "        if self.trans:\n",
    "            img = self.trans(img)\n",
    "        label = np.sum(np.eye(374)[label], axis=0)\n",
    "        return img, label.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.num\n",
    "    \n",
    "    def _gen_noise_image(self, image, noise_rate):\n",
    "        noise_image = np.random.uniform(-0.001, 0.001,(image.shape)).astype('float32')\n",
    "        return noise_rate * noise_image + (1-noise_rate) * image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckX(nn.Module):\n",
    "    expansion = 2\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=4):\n",
    "        super(BottleneckX, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False, groups=groups)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 2, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # SE\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv_down = nn.Conv2d(\n",
    "            planes * 2, planes // 8, kernel_size=1, bias=False)\n",
    "        self.conv_up = nn.Conv2d(\n",
    "            planes // 8, planes * 2, kernel_size=1, bias=False)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        # Downsample\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out1 = self.global_pool(out)\n",
    "        out1 = self.conv_down(out1)\n",
    "        out1 = self.relu(out1)\n",
    "        out1 = self.conv_up(out1)\n",
    "        out1 = self.sig(out1)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        res = out1 * out + residual\n",
    "        res = self.relu(res)\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "class SEResNeXt(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=375):\n",
    "        self.inplanes = 64\n",
    "        super(SEResNeXt, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 128, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 256, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 512, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 1024, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d((6, 6))\n",
    "        self.fc = nn.Linear(1024 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "NUM_TRAIN = len(train_pairs)\n",
    "NUM_TEST = len(test_pairs)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((192, 192)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(degrees=90),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.3853909028535724, 0.4004333749569167, 0.34717936323577203], [1,1,1]),\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((192, 192)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.3853909028535724, 0.4004333749569167, 0.34717936323577203], [1,1,1]),\n",
    "])\n",
    "\n",
    "trainDataset = COREL_5K(train_pairs, NUM_TRAIN, train_transform)\n",
    "train_loader = DataLoader(dataset=trainDataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
    "\n",
    "valDataset = COREL_5K(val_pairs, NUM_TEST, test_transform)\n",
    "val_loader = DataLoader(dataset=valDataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n",
    "\n",
    "testDataset = COREL_5K(test_pairs, NUM_TEST, test_transform)\n",
    "test_loader = DataLoader(dataset=testDataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {}\n",
    "labels = []\n",
    "[labels.extend(i[1]) for i in train_pairs]\n",
    "[labels.extend(i[1]) for i in test_pairs]\n",
    "[labels.extend(i[1]) for i in val_pairs]\n",
    "for i in labels:\n",
    "    if i in a.keys():\n",
    "        a[i] += 1\n",
    "    else:\n",
    "        a[i] = 1\n",
    "for i in a.keys():\n",
    "    a[i] = 1/a[i]\n",
    "# weights = torch.FloatTensor(list(a.values())).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "model = SEResNeXt(BottleneckX, [3, 4, 6, 3], num_classes=374)\n",
    "model.cuda()\n",
    "critrien = nn.BCEWithLogitsLoss(size_average=False)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|‚ñç            | 15/500 [00:07<04:15,  1.90b/s]Process Process-7:\n",
      "Process Process-8:\n",
      "Process Process-5:\n",
      "Process Process-6:\n",
      "                                                  Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 249, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 343, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 34, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 342, in get\n",
      "    with self._rlock:\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/usr/lib/python3.5/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6024e21a7dc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mtrain_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "NUM_EPOCHS = 10\n",
    "best_acc = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    train_acc = 0\n",
    "    test_acc = 0\n",
    "    model.train()\n",
    "    for i, (data, label) in tqdm(enumerate(train_loader), total=NUM_TRAIN // BATCH_SIZE, ncols=50, leave=False, unit='b'):\n",
    "        data = Variable(data).cuda()\n",
    "        label = Variable(label).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = critrien(output, label)\n",
    "        train_loss += loss.data[0]\n",
    "        _, predict = torch.max(output, 1)\n",
    "        label = label.cpu().data.numpy()\n",
    "        pred = predict.data\n",
    "        for i in range(len(pred)):\n",
    "            if pred[i] in list(np.where(label[i]==1)[0]):\n",
    "                train_acc += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    for i, (data, label) in enumerate(val_loader):\n",
    "        data = Variable(data).cuda()\n",
    "        label = Variable(label).cuda()\n",
    "        output = model(data)\n",
    "        loss = critrien(output, label)\n",
    "        test_loss += loss.data[0]\n",
    "        _, predict = torch.max(output, 1)\n",
    "        label = label.cpu().data.numpy()\n",
    "        pred = predict.data\n",
    "        for i in range(len(pred)):\n",
    "            if pred[i] in list(np.where(label[i]==1)[0]):\n",
    "                test_acc += 1\n",
    "    \n",
    "    print('Epoch [%d/%d], Train Loss: %.4f, Train Acc: %.4f, Test Loss: %.4f, Test Acc: %.4f'\n",
    "            %(epoch+1, NUM_EPOCHS, \n",
    "              train_loss / NUM_TRAIN, train_acc / NUM_TRAIN, \n",
    "              test_loss / NUM_TEST, test_acc / NUM_TEST))\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        torch.save(model.state_dict(), \"models/SEResNext2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Traceback (most recent call last):\n  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 109, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 109, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 95, in default_collate\n    return torch.stack([torch.from_numpy(b) for b in batch], 0)\n  File \"/usr/local/lib/python3.5/dist-packages/torch/functional.py\", line 64, in stack\n    return torch.cat(inputs, dim)\nRuntimeError: inconsistent tensor sizes at /pytorch/torch/lib/TH/generic/THTensorMath.c:2709\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-3d5645dd2a72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 40, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 109, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 109, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"/usr/local/lib/python3.5/dist-packages/torch/utils/data/dataloader.py\", line 95, in default_collate\n    return torch.stack([torch.from_numpy(b) for b in batch], 0)\n  File \"/usr/local/lib/python3.5/dist-packages/torch/functional.py\", line 64, in stack\n    return torch.cat(inputs, dim)\nRuntimeError: inconsistent tensor sizes at /pytorch/torch/lib/TH/generic/THTensorMath.c:2709\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "def predict():\n",
    "    with open(\"Datasets/corel_5k/labels/words\") as f:\n",
    "        words = [i[:-1] for i in f.readlines()]\n",
    "\n",
    "    def img_back(img):\n",
    "        mean = [0.3853909028535724, 0.4004333749569167, 0.34717936323577203]\n",
    "        img[:, :, 0] = img[:, :, 0] + mean[0]\n",
    "        img[:, :, 1] = img[:, :, 1] + mean[1]\n",
    "        img[:, :, 2] = img[:, :, 2] + mean[2]\n",
    "        return img\n",
    "\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_TEST = len(test_pairs)\n",
    "\n",
    "    testDataset = COREL_5K(test_pairs, NUM_TEST)\n",
    "    test_loader = DataLoader(dataset=testDataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=False)\n",
    "    model = SEResNeXt(BottleneckX, [3, 4, 6, 3], num_classes=374)\n",
    "    model.load_state_dict(torch.load(\"models/SEResNext1.pkl\"))\n",
    "    model.cuda()\n",
    "\n",
    "    test_acc = 0\n",
    "    model.eval()\n",
    "    for i, (data, label) in enumerate(test_loader):\n",
    "        data = Variable(data).cuda()\n",
    "        label = Variable(label).cuda()\n",
    "        output = model(data)\n",
    "    #     _, predict = torch.max(output, 1)\n",
    "        _, predict = torch.sort(output)\n",
    "        label = label.cpu().data.numpy()\n",
    "        pred = (predict.data)[:, -4:]\n",
    "        for i in range(len(pred)):\n",
    "            if len(set(pred[i]) & set(list(np.where(label[i]==1)[0]))):\n",
    "                test_acc += 1\n",
    "            else:\n",
    "                img = data.cpu().data.numpy()[i]\n",
    "                gt = [words[i] for i in list(np.where(label[i]==1)[0])]\n",
    "                predic = [words[i] for i in list(pred[i].cpu().numpy())]\n",
    "                viz.image(np.transpose(img_back(img), (2, 0, 1)),\n",
    "                         opts=dict(title=\" \".join(gt), caption=\" \".join(predic)))\n",
    "    print(test_acc / NUM_TEST)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
